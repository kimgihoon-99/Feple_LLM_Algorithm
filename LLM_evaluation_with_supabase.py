import os
from dotenv import load_dotenv
import pandas as pd
import numpy as np
import sys
from openai import OpenAI
from supabase import create_client, Client
import json
from datetime import datetime
import time

# absolute_grading ëª¨ë“ˆë“¤ import
sys.path.append('./absolute_grading')
from grade_politeness_auto import get_politeness_results
from grade_empathy_auto import get_empathy_results
from grade_emotional_stability_auto import get_emotional_stability_results
from grade_stability_auto import get_stability_results
from grade_problem_solving import get_problem_solving_results

# === ê° ì§€í‘œë³„ metrics dict â†’ ì ìˆ˜/ë“±ê¸‰ í•¨ìˆ˜ ===
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

def minmax_normalize(value, min_val, max_val):
    if max_val > min_val:
        return (value - min_val) / (max_val - min_val)
    else:
        return 0.5

def compute_politeness_score_and_grade(metrics):
    path = os.path.join(BASE_DIR, 'cutoff', 'grade_cutoff_politeness.json')
    with open(path) as f:
        cutoff_json = json.load(f)
        cutoffs = cutoff_json['cutoff']
        minmax = cutoff_json['minmax']
    hr = minmax_normalize(metrics['honorific_ratio'], minmax['honorific_ratio']['min'], minmax['honorific_ratio']['max'])
    pr = minmax_normalize(metrics['positive_word_ratio'], minmax['positive_word_ratio']['min'], minmax['positive_word_ratio']['max'])
    er = minmax_normalize(metrics['euphonious_word_ratio'], minmax['euphonious_word_ratio']['min'], minmax['euphonious_word_ratio']['max'])
    nr = minmax_normalize(metrics['negative_word_ratio'], minmax['negative_word_ratio']['min'], minmax['negative_word_ratio']['max'])
    score = (hr + pr + er + (1 - nr)) / 4
    if score >= cutoffs["A"]: grade = "A"
    elif score >= cutoffs["B"]: grade = "B"
    elif score >= cutoffs["C"]: grade = "C"
    elif score >= cutoffs["D"]: grade = "D"
    elif score >= cutoffs["E"]: grade = "E"
    elif score >= cutoffs["F"]: grade = "F"
    else: grade = "G"
    return score, grade

def compute_empathy_score_and_grade(metrics):
    path = os.path.join(BASE_DIR, 'cutoff', 'grade_cutoff_empathy.json')
    with open(path) as f:
        cutoff_json = json.load(f)
        cutoffs = cutoff_json['cutoff']
        minmax = cutoff_json['minmax']
    er = minmax_normalize(metrics['empathy_ratio'], minmax['empathy_ratio']['min'], minmax['empathy_ratio']['max'])
    ar = minmax_normalize(metrics['apology_ratio'], minmax['apology_ratio']['min'], minmax['apology_ratio']['max'])
    score = er * 0.7 + ar * 0.3
    if score >= cutoffs["A"]: grade = "A"
    elif score >= cutoffs["B"]: grade = "B"
    elif score >= cutoffs["C"]: grade = "C"
    elif score >= cutoffs["D"]: grade = "D"
    elif score >= cutoffs["E"]: grade = "E"
    elif score >= cutoffs["F"]: grade = "F"
    else: grade = "G"
    return score, grade

def compute_problem_solving_score_and_grade(metrics):
    # ë¬¸ì œí•´ê²°ì€ ì´ì‚°í˜• ì ìˆ˜ ë§¤í•‘
    score = float(metrics['suggestions'])
    if score == 1.0:
        grade = "A"
    elif score == 0.6:
        grade = "B"
    elif score == 0.2:
        grade = "C"
    elif score == 0.0:
        grade = "D"
    else:
        grade = "Invalid"
    return score, grade

def compute_emotional_stability_score_and_grade(metrics):
    path = os.path.join(BASE_DIR, 'cutoff', 'grade_cutoff_emotional_stability.json')
    with open(path) as f:
        cutoff_json = json.load(f)
        cutoffs = cutoff_json['cutoff']
        minmax = cutoff_json['minmax']
    early = minmax_normalize(metrics['customer_sentiment_early'], minmax['customer_sentiment_early']['min'], minmax['customer_sentiment_early']['max'])
    late = minmax_normalize(metrics['customer_sentiment_late'], minmax['customer_sentiment_late']['min'], minmax['customer_sentiment_late']['max'])
    change = late - early
    if change == 0:
        if early < 0.4:
            raw = 0.50
        elif early >= 0.7:
            raw = 0.95
        else:
            raw = 0.85
    else:
        improvement = max(change, 0.0)
        raw = late * 0.7 + improvement * 0.3
    score = max(0.0, min(raw, 1.0))
    if score >= cutoffs["A"]: grade = "A"
    elif score >= cutoffs["B"]: grade = "B"
    elif score >= cutoffs["C"]: grade = "C"
    elif score >= cutoffs["D"]: grade = "D"
    elif score >= cutoffs["E"]: grade = "E"
    elif score >= cutoffs["F"]: grade = "F"
    else: grade = "G"
    return score, grade

def compute_stability_score_and_grade(metrics):
    path = os.path.join(BASE_DIR, 'cutoff', 'grade_cutoff_stability.json')
    with open(path) as f:
        cutoff_json = json.load(f)
        cutoffs = cutoff_json['cutoff']
        minmax = cutoff_json['minmax']
    ic = minmax_normalize(metrics['interruption_count'], minmax['interruption_count']['min'], minmax['interruption_count']['max'])
    sr = minmax_normalize(metrics['silence_ratio'], minmax['silence_ratio']['min'], minmax['silence_ratio']['max'])
    tr = minmax_normalize(metrics['talk_ratio'], minmax['talk_ratio']['min'], minmax['talk_ratio']['max'])
    interrupt_score = 1 - ic
    optimal_silence = 0.25
    silence_distance = abs(sr - optimal_silence)
    silence_score = max(0.0, 1 - 4 * silence_distance)
    talk_distance = abs(tr - 0.5)
    talk_score = max(0.0, 1 - 2 * talk_distance)
    score = interrupt_score * 0.3 + silence_score * 0.4 + talk_score * 0.3
    score = float(np.clip(score, 0.0, 1.0))
    if score >= cutoffs["A"]: grade = "A"
    elif score >= cutoffs["B"]: grade = "B"
    elif score >= cutoffs["C"]: grade = "C"
    elif score >= cutoffs["D"]: grade = "D"
    elif score >= cutoffs["E"]: grade = "E"
    elif score >= cutoffs["F"]: grade = "F"
    else: grade = "G"
    return score, grade

# === metrics dict â†’ 5ëŒ€ ì§€í‘œ ì ìˆ˜/ë“±ê¸‰ dict ===
def metrics_to_scores_and_grades(metrics):
    politeness_score, politeness_grade = compute_politeness_score_and_grade(metrics)
    empathy_score, empathy_grade = compute_empathy_score_and_grade(metrics)
    problem_score, problem_grade = compute_problem_solving_score_and_grade(metrics)
    emotional_score, emotional_grade = compute_emotional_stability_score_and_grade(metrics)
    stability_score, stability_grade = compute_stability_score_and_grade(metrics)
    return {
        "Politeness": {"score": politeness_score, "grade": politeness_grade},
        "Empathy": {"score": empathy_score, "grade": empathy_grade},
        "ProblemSolving": {"score": problem_score, "grade": problem_grade},
        "EmotionalStability": {"score": emotional_score, "grade": emotional_grade},
        "Stability": {"score": stability_score, "grade": stability_grade}
    }

# .env íŒŒì¼ì—ì„œ í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
def init_supabase():
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("[WARNING] Supabase ì—°ê²° ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ê²°ê³¼ëŠ” ë¡œì»¬ì—ë§Œ ì¶œë ¥ë©ë‹ˆë‹¤.")
        return None
    
    try:
        supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        print("[INFO] Supabase ì—°ê²° ì„±ê³µ!")
        return supabase
    except Exception as e:
        print(f"[ERROR] Supabase ì—°ê²° ì‹¤íŒ¨: {e}")
        return None

supabase = init_supabase()

# Supabaseì—ì„œ analysis_results í…Œì´ë¸”ì˜ ë¯¸ì²˜ë¦¬ rowë¥¼ pollingí•˜ì—¬ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜

def get_unprocessed_analysis_results():
    if not supabase:
        print("[ERROR] Supabase ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        return []
    # ì´ë¯¸ í‰ê°€ëœ session_id ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    evaluated = supabase.table("counselor_evaluations").select("session_id").execute().data
    evaluated_ids = set(row["session_id"] for row in evaluated)
    # analysis_resultsì—ì„œ í‰ê°€ ì•ˆëœ rowë§Œ ê°€ì ¸ì˜¤ê¸°
    analysis_rows = supabase.table("analysis_results").select("*").execute().data
    return [row for row in analysis_rows if row["session_id"] not in evaluated_ids]

# 1. ë°ì´í„° ë¡œë“œ (new_data.csv ìš°ì„ , ì—†ìœ¼ë©´ dummy_data.csv)
DATA_PATH = 'data/new_data.csv'
DUMMY_PATH = 'data/dummy_data.csv'

if os.path.exists(DATA_PATH):
    print(f"[INFO] new_data.csvë¡œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    df = pd.read_csv(DATA_PATH)
    data_source = "new_data.csv"
else:
    print(f"[INFO] new_data.csvê°€ ì—†ì–´ dummy_data.csvë¡œ í‰ê°€ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.")
    df = pd.read_csv(DUMMY_PATH)
    data_source = "dummy_data.csv"

df.columns = df.columns.str.strip()

# session_idê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„±
def ensure_session_id(df):
    if 'session_id' not in df.columns:
        df = df.copy()
        df['session_id'] = [f'session_{i+1:03d}' for i in range(len(df))]
    return df

df = ensure_session_id(df)

# 2. ê²°ê³¼ ì €ì¥ ë¦¬ìŠ¤íŠ¸
eval_results = []

# 3. OpenAI ëª¨ë¸ ì„¤ì •
MODEL_NAME = "gpt-4o-mini"

# 4. absolute_grading ì‹œìŠ¤í…œì—ì„œ í‰ê°€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
print("[INFO] absolute_grading ì‹œìŠ¤í…œì—ì„œ í‰ê°€ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

# === ìƒˆ ìë™í™” íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤ ===

def run_llm_evaluation_with_scores(scores, transcript):
    prompt = f"""
    ìƒë‹´ì‚¬ 5ëŒ€ì§€í‘œ í‰ê°€ ê²°ê³¼:
    - ì •ì¤‘í•¨: {scores['Politeness']['score']:.3f}ì  ({scores['Politeness']['grade']}ë“±ê¸‰)
    - ê³µê°: {scores['Empathy']['score']:.3f}ì  ({scores['Empathy']['grade']}ë“±ê¸‰)
    - ë¬¸ì œí•´ê²°: {scores['ProblemSolving']['score']:.3f}ì  ({scores['ProblemSolving']['grade']}ë“±ê¸‰)
    - ê°ì •ì•ˆì •ì„±: {scores['EmotionalStability']['score']:.3f}ì  ({scores['EmotionalStability']['grade']}ë“±ê¸‰)
    - ëŒ€í™”íë¦„: {scores['Stability']['score']:.3f}ì  ({scores['Stability']['grade']}ë“±ê¸‰)

    ëŒ€í™”: {transcript}

    ê°•ì (ìƒìœ„ 2ê°œ ì§€í‘œ), ì•½ì (í•˜ìœ„ 2ê°œ ì§€í‘œ), ì½”ì¹­ë©˜íŠ¸ë¥¼ ê°„ê²°í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.
    """
    try:
        response = openai_client.chat.completions.create(
            model=MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1500,
            temperature=0.7
        )
        feedback = response.choices[0].message.content
    except Exception as e:
        feedback = f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}"
    return feedback

def save_analysis_feedback_to_supabase(row, scores, feedback):
    if not supabase:
        return False
    try:
        data = {
            "session_id": row["session_id"],
            "politeness_score": scores["Politeness"]["score"],
            "politeness_grade": scores["Politeness"]["grade"],
            "empathy_score": scores["Empathy"]["score"],
            "empathy_grade": scores["Empathy"]["grade"],
            "problem_solving_score": scores["ProblemSolving"]["score"],
            "problem_solving_grade": scores["ProblemSolving"]["grade"],
            "emotional_stability_score": scores["EmotionalStability"]["score"],
            "emotional_stability_grade": scores["EmotionalStability"]["grade"],
            "stability_score": scores["Stability"]["score"],
            "stability_grade": scores["Stability"]["grade"],
            "gpt_feedback": feedback,
            "evaluation_model": MODEL_NAME,
            "data_source": "analysis_results"
        }
        result = supabase.table('counselor_evaluations').insert(data).execute()
        if result.data:
            print(f"[SUPABASE] ì„¸ì…˜ {row['session_id']} ë°ì´í„° ì €ì¥ ì„±ê³µ!")
            return True
        else:
            print(f"[SUPABASE] ì„¸ì…˜ {row['session_id']} ë°ì´í„° ì €ì¥ ì‹¤íŒ¨")
            return False
    except Exception as e:
        print(f"[SUPABASE ERROR] ì„¸ì…˜ {row['session_id']} ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
        return False

def main():
    print("[ìë™í™”] Supabase analysis_results â†’ ì‚°ì‹ ì ìˆ˜/ë“±ê¸‰ â†’ LLM í‰ê°€ â†’ counselor_evaluations ì €ì¥ íŒŒì´í”„ë¼ì¸ ì‹œì‘!")
    while True:
        unprocessed_rows = get_unprocessed_analysis_results()
        print(f"[ìë™í™”] ì²˜ë¦¬í•  row ê°œìˆ˜: {len(unprocessed_rows)}")
        for row in unprocessed_rows:
            metrics = row.get("metrics")
            transcript = row.get("transcript")
            scores = metrics_to_scores_and_grades(metrics)
            feedback = run_llm_evaluation_with_scores(scores, transcript)
            save_analysis_feedback_to_supabase(row, scores, feedback)
            print(f"[ìë™í™”] session_id {row['session_id']} ì²˜ë¦¬ ì™„ë£Œ")
        time.sleep(10)

if __name__ == "__main__":
    main()

# 6. ê° ì„¸ì…˜ë³„ ë°˜ë³µ ì²˜ë¦¬ (rowë³„ë¡œ ê²°ê³¼ ì¶”ì¶œ)
# for idx, row in df.iterrows():
#     session_id = row['session_id']
#     evaluation_result = {
#         "Politeness": {
#             "score": float(politeness_result['Politeness_score'].iloc[idx]),
#             "grade": politeness_result['Politeness_Grade'].iloc[idx]
#         },
#         "Empathy": {
#             "score": float(empathy_result['Empathy_score'].iloc[idx]),
#             "grade": empathy_result['Empathy_Grade'].iloc[idx]
#         },
#         "ProblemSolving": {
#             "score": float(problem_result['ProblemSolving_score'].iloc[idx]),
#             "grade": problem_result['ProblemSolving_Grade'].iloc[idx]
#         },
#         "EmotionalStability": {
#             "score": float(emotional_result['EmotionalStability_score'].iloc[idx]),
#             "grade": emotional_result['EmotionalStability_Grade'].iloc[idx]
#         },
#         "Stability": {
#             "score": float(stability_result['Stability_score'].iloc[idx]),
#             "grade": stability_result['Stability_Grade'].iloc[idx]
#         }
#     }

#     # OpenAI í”„ë¡¬í”„íŠ¸ ìƒì„±
#     prompt = f"""
# ë‹¹ì‹ ì€ ìƒë‹´ì‚¬ êµìœ¡ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ 5ê°€ì§€ ì§€í‘œ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ í”¼ë“œë°±ì„ ì œê³µí•´ ì£¼ì„¸ìš”.

# ğŸ“Š **í‰ê°€ ê²°ê³¼**
# - ì •ì¤‘í•¨: {evaluation_result['Politeness']['score']:.3f}ì  ({evaluation_result['Politeness']['grade']}ë“±ê¸‰)
# - ê³µê°: {evaluation_result['Empathy']['score']:.3f}ì  ({evaluation_result['Empathy']['grade']}ë“±ê¸‰)  
# - ë¬¸ì œí•´ê²°: {evaluation_result['ProblemSolving']['score']:.3f}ì  ({evaluation_result['ProblemSolving']['grade']}ë“±ê¸‰)
# - ê°ì •ì•ˆì •ì„±: {evaluation_result['EmotionalStability']['score']:.3f}ì  ({evaluation_result['EmotionalStability']['grade']}ë“±ê¸‰)
# - ëŒ€í™”íë¦„: {evaluation_result['Stability']['score']:.3f}ì  ({evaluation_result['Stability']['grade']}ë“±ê¸‰)

# **ì¶œë ¥ í˜•ì‹ (ê°„ê²°í•˜ê²Œ):**

# ** ê°•ì  (ìƒìœ„ 2ê°œ ì§€í‘œ)**
# 1. [ì§€í‘œëª…] (ì ìˆ˜, ë“±ê¸‰): í•œ ì¤„ ì„¤ëª…
# 2. [ì§€í‘œëª…] (ì ìˆ˜, ë“±ê¸‰): í•œ ì¤„ ì„¤ëª…

# ** ì•½ì  (í•˜ìœ„ 2ê°œ ì§€í‘œ)**  
# 1. [ì§€í‘œëª…] (ì ìˆ˜, ë“±ê¸‰): í•œ ì¤„ ì„¤ëª…
# 2. [ì§€í‘œëª…] (ì ìˆ˜, ë“±ê¸‰): í•œ ì¤„ ì„¤ëª…

# ** ì½”ì¹­ ë©˜íŠ¸**
# ê°•ì ê³¼ ì•½ì ì„ í™œìš©í•œ 3-4ì¤„ì˜ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ê°œì„  ë°©ì•ˆ ì œì‹œ
# """
#     try:
#         response = openai_client.chat.completions.create(
#             model=MODEL_NAME,
#             messages=[
#                 {"role": "user", "content": prompt}
#             ],
#             max_tokens=1500,
#             temperature=0.7
#         )
#         feedback = response.choices[0].message.content
#     except Exception as e:
#         feedback = f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}"

#     # Supabaseì— ì €ì¥
#     save_success = save_to_supabase(session_id, evaluation_result, feedback, data_source)
    
#     eval_results.append({
#         'session_id': session_id,
#         'evaluation': evaluation_result,
#         'feedback': feedback,
#         'saved_to_supabase': save_success
#     })
#     print(f"[ì„¸ì…˜ {session_id}] ë¶„ì„ ì™„ë£Œ! {'(Supabase ì €ì¥ ì„±ê³µ)' if save_success else '(ë¡œì»¬ë§Œ ì €ì¥)'}")

# 7. ì „ì²´ ê²°ê³¼ ì¶œë ¥
# print(f"\n=== ì „ì²´ ì„¸ì…˜ ë¶„ì„ ê²°ê³¼ (ì´ {len(eval_results)}ê°œ ì„¸ì…˜) ===")
# saved_count = sum(1 for r in eval_results if r.get('saved_to_supabase', False))
# print(f"ğŸ“Š Supabase ì €ì¥: {saved_count}/{len(eval_results)}ê°œ ì„¸ì…˜ ì„±ê³µ")

# for r in eval_results:
#     print(f"\n[ì„¸ì…˜ ID: {r['session_id']}]")
#     print("ğŸ“Š ì‹¤ì œ í‰ê°€ ê²°ê³¼:")
#     for key, value in r['evaluation'].items():
#         print(f"  {key}: ì ìˆ˜ {value['score']:.3f}, ë“±ê¸‰ {value['grade']}")
#     print("-" * 40)
#     print("ğŸ¤– OpenAI GPT í”¼ë“œë°±:")
#     print(r['feedback'])
#     if r.get('saved_to_supabase'):
#         print("âœ… Supabaseì— ì €ì¥ ì™„ë£Œ")
#     else:
#         print("âŒ Supabase ì €ì¥ ì‹¤íŒ¨")
#     print("=" * 60)

# 8. ì €ì¥ëœ ë°ì´í„° ìš”ì•½ ì •ë³´
# if supabase and saved_count > 0:
#     print(f"\nğŸ¯ **Supabase ì €ì¥ ì™„ë£Œ!**")
#     print(f"- í…Œì´ë¸”: counselor_evaluations")
#     print(f"- ì €ì¥ëœ ì„¸ì…˜: {saved_count}ê°œ")
#     print(f"- ë°ì´í„° ì†ŒìŠ¤: {data_source}")
#     print(f"- í‰ê°€ ëª¨ë¸: {MODEL_NAME}")
#     print(f"- ì €ì¥ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}") 